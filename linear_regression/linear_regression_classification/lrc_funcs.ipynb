{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "sVE3pQHj3Doi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X_SPQT3f2MDx"
      },
      "outputs": [],
      "source": [
        "def dummy(parameter, i):\n",
        "  dummy_parameter = parameter + i\n",
        "  return dummy_parameter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize(dimension):\n",
        "  w = np.full((dimension, 1), 0.01)\n",
        "  b = 0\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "7DSUyfsY2hAF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  y_head = 1 / (1 + np.exp(-z))\n",
        "  return y_head"
      ],
      "metadata": {
        "id": "1PiGOL693fGn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(w, b, x_train, y_train):\n",
        "  z = np.dot(w.T, x_train) + b\n",
        "  y_head = sigmoid(z)\n",
        "  loss = -(1-y_train)*np.log(1-y_head)-y_train*np.log(y_head)\n",
        "  cost = np.sum(loss) / x_train.shape[1] #x_train.shape[1] for scaling\n",
        "  return cost"
      ],
      "metadata": {
        "id": "Vr0boUsL3rq0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward(w, b,  x_train, y_train):\n",
        "  #forward part\n",
        "  z = np.dot(w.T, x_train) + b\n",
        "  y_head = sigmoid(z)\n",
        "  loss = -(1-y_train)*np.log(1-y_head)-y_train*np.log(y_head)\n",
        "  cost = np.sum(loss) / x_train.shape[1]\n",
        "\n",
        "  #backward part\n",
        "  derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) / x_train.shape[1]\n",
        "  derivative_bias = np.sum(y_head - y_train) / x_train.shape[1]\n",
        "  gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
        "  return cost, gradients"
      ],
      "metadata": {
        "id": "1AlQeXmi5due"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n",
        "  cost_list = []\n",
        "  cost_list2 = []\n",
        "  index = []\n",
        "\n",
        "  for i in range(number_of_iteration):\n",
        "    cost, gradients = forward_backward(w, b, x_train, y_train)\n",
        "    cost_list.append(cost)\n",
        "    w = w - learning_rate * gradients[\"derivative_weight\"]\n",
        "    b = b - learning_rate * gradients[\"derivative_bias\"]\n",
        "    if i % 10 == 0:\n",
        "      cost_list2.append(cost)\n",
        "      index.append(i)\n",
        "      print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "      parameters = {\"weight\": w, \"bias\": b}\n",
        "\n",
        "  return parameters, cost_list, cost_list2, index"
      ],
      "metadata": {
        "id": "UkXCdv4k5qbX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, x_test):\n",
        "  z = sigmoid(np.dot(w.T, x_test) + b)\n",
        "  y_prediction = np.zeros((1, x_test.shape[1]))\n",
        "\n",
        "  #according to our example if z>0.5 ➞ sign 1, if z<0.5 ➞ sign 0\n",
        "\n",
        "  for i in range(z.shape[i]):\n",
        "    if z[0,i] <= 0.5:\n",
        "      y_prediction[0, i] = 0\n",
        "    else:\n",
        "      y_prediction[0, i] = 1\n",
        "  return y_prediction"
      ],
      "metadata": {
        "id": "Jocx1SHN6rQM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_reg(x_train, y_train, x_test, y_test, learning_rate, number_of_iteration):\n",
        "  dimension = x_train.shape[0]\n",
        "  w, b = initialize(dimension)\n",
        "  parameters, cost_list, cost_list2, index = update(w, b, x_train, y_train, learning_rate, number_of_iteration)\n",
        "  y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n",
        "  y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n",
        "  print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
        "  print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
      ],
      "metadata": {
        "id": "WqHK-ae57lL6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression with sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(random_state = 19, max_iter = 150)\n",
        "print(\"train accuracy: {}\".format(log_reg.fit(x_train, y_train).score(x_train, y_train)))\n",
        "print(\"train accuracy: {}\".format(log_reg.fit(x_test, y_test).score(x_test, y_test)))\n"
      ],
      "metadata": {
        "id": "MMfVDlC878GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qisE72_48ylb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}